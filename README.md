# Iterative Meta-Game Analysis

A framework for empirical game-theoretic analysis (EGTA) of multi-agent systems. This framework provides rigorous evaluation of policies in meta-games across three levels of analysis, with bootstrap uncertainty quantification.

## Installation

Requires Python 3.10+. Install using [uv](https://github.com/astral-sh/uv):

```bash
# Clone the repository
git clone https://github.com/your-username/Causal-Game-Analysis.git
cd Causal-Game-Analysis

# Sync dependencies (creates venv automatically)
uv sync

# For development (includes testing and linting tools)
uv sync --extra dev
```

## Quick Start

### Basic Usage

```python
import pandas as pd
import numpy as np
from iterative_game_analysis import (
    MetaGame,
    Bootstrap,
    level1_analysis,
    ecosystem_lift,
    shapley_value,
)

# Create or load bargaining data
# Format: DataFrame with columns for both players' payoffs, BATNAs, and EF1
df = pd.DataFrame([
    {"policy_i": "alice", "policy_j": "bob",
     "payoff_i": 80, "payoff_j": 75,
     "batna_i": 50, "batna_j": 45, "ef1": 1},
    {"policy_i": "alice", "policy_j": "carol",
     "payoff_i": 60, "payoff_j": 70,
     "batna_i": 50, "batna_j": 40, "ef1": 1},
    {"policy_i": "bob", "policy_j": "alice",
     "payoff_i": 75, "payoff_j": 80,
     "batna_i": 45, "batna_j": 50, "ef1": 1},
    # ... more bargaining instances
])

# Build meta-game from raw data (uses payoff_i for payoff matrix)
game = MetaGame.from_dataframe(df, outcome_col="payoff_i")

# Compute equilibrium
sigma = game.solve("mene")  # Max-entropy Nash equilibrium
print("Equilibrium:", dict(zip(game.policies, sigma)))
```

### Level 1: Partner Lift Analysis

```python
# Evaluate a candidate policy against a baseline
result = level1_analysis(
    metagame=game,
    baseline_policies=["alice", "bob"],
    candidate="carol",
    solver="mene"
)

print("Partner Lift per incumbent:", result["per_incumbent"])
print("Uniform average:", result["uniform_avg"])
print("Equilibrium-weighted average:", result["equilibrium_avg"])
print("Worst-case:", result["min"])
print("Best-case:", result["max"])
```

### Level 2: Ecosystem Lift Analysis

```python
# Measure ecosystem impact with re-equilibration
result = ecosystem_lift(
    metagame=game,
    baseline_policies=["alice", "bob"],
    candidate="carol",
    solver="mene",
    welfare_fn="utilitarian"  # or "nash", "egalitarian"
)

print("Ecosystem lift:", result["delta_eco"])
print("Entry mass:", result["entry_mass"])
print("Equilibrium shift:", result["equilibrium_shift"])
print("Incumbent value shifts:", result["incumbent_shifts"])
```

### Level 3: Shapley Attribution

```python
# Compute Shapley values for ecosystem attribution
def value_fn(policies):
    if len(policies) < 2:
        return 0.0
    sub_game = game.subset(policies)
    sigma = sub_game.solve("mene")
    return sub_game.welfare(sigma, "utilitarian")

shapley = shapley_value(game.policies, value_fn)
print("Shapley values:", shapley)

# Or Banzhaf values
from iterative_game_analysis import banzhaf_value
banzhaf = banzhaf_value(game.policies, value_fn)
```

### Bootstrap for Full Analysis

```python
# Bargaining data format: one row per negotiation instance
df = pd.DataFrame([
    {"policy_i": "gpt4", "policy_j": "claude",
     "payoff_i": 85, "payoff_j": 90,
     "batna_i": 50, "batna_j": 55, "ef1": 1},
    {"policy_i": "gpt4", "policy_j": "llama",
     "payoff_i": 70, "payoff_j": 65,
     "batna_i": 50, "batna_j": 45, "ef1": 0},
    # ... more bargaining instances
])

# Bootstrap with full L1/L2/L3 analysis
bootstrap = Bootstrap(
    df, n_samples=1000, seed=42,
    payoff_i_col="payoff_i", payoff_j_col="payoff_j",
    batna_i_col="batna_i", batna_j_col="batna_j",
    ef1_col="ef1",
)

# Run complete analysis on each bootstrap sample
results = bootstrap.run_full_analysis(
    include_l3=True,
    l3_method="both",  # Shapley and Banzhaf
    progress=True,
)

# Each result contains: l1, l2, l3, matrices, full_game
# Access per-agent leave-one-out metrics
for agent in bootstrap.policies:
    l1 = results[0]["l1"][agent]
    l2 = results[0]["l2"][agent]
    print(f"{agent}: L1 lift={l1['uniform_avg']:.3f}, L2 delta_eco={l2['delta_eco']['uw']:.3f}")

# Welfare metrics (UW, NW, NW+) computed at equilibrium
print("Full game welfare:", results[0]["full_game"]["welfare"])
print("Full game EF1:", results[0]["full_game"]["ef1"])

# Get confidence intervals across bootstrap samples
l1_lifts = [r["l1"]["gpt4"]["uniform_avg"] for r in results]
lower, median, upper = Bootstrap.confidence_interval(l1_lifts, alpha=0.05)
print(f"95% CI for GPT4 L1 lift: [{lower:.3f}, {upper:.3f}]")
```

### EF1 Fairness Analysis (for Bargaining)

```python
from iterative_game_analysis import ef1_frequency_matrix, aggregate_ef1_between_groups

# EF1 frequency matrix from bargaining data
ef1_matrix, policies = ef1_frequency_matrix(df)

# Compare EF1 between policy groups (e.g., LLM providers)
ef1_stats = aggregate_ef1_between_groups(
    df,
    group_a=["gpt4", "claude"],
    group_b=["llama", "mistral"]
)
print("EF1 frequency (Group A vs B):", ef1_stats["a_vs_b"])
print("EF1 frequency (within Group A):", ef1_stats["within_a"])

# EF1 is also computed at equilibrium in run_full_analysis()
results = bootstrap.run_full_analysis(include_l3=False)
print("EF1 at equilibrium:", results[0]["full_game"]["ef1"])
```

### Direct Matrix Construction

```python
# If you already have a payoff matrix
payoff_matrix = np.array([
    [0.5, 0.0, 1.0],  # rock
    [1.0, 0.5, 0.0],  # paper
    [0.0, 1.0, 0.5],  # scissors
])

game = MetaGame(
    policies=["rock", "paper", "scissors"],
    payoff_matrix=payoff_matrix
)
```

## API Reference

### Core Classes

| Class | Description |
|-------|-------------|
| `MetaGame` | Empirical meta-game representation with payoff matrix |
| `Bootstrap` | Bootstrap resampling with full L1/L2/L3 analysis |

### Bootstrap Methods

| Method | Description |
|--------|-------------|
| `run_full_analysis()` | Run complete L1/L2/L3 analysis on each bootstrap sample |
| `run()` | Run custom analysis function on each bootstrap sample |
| `sample()` | Generate one bootstrap sample (stratified by policy pair) |
| `confidence_interval()` | Compute percentile CI from bootstrap distribution |

### Analysis Functions

| Function | Level | Description |
|----------|-------|-------------|
| `level1_analysis()` | 1 | Partner Lift (no re-equilibration) |
| `ecosystem_lift()` | 2 | Ecosystem Lift (with re-equilibration) |
| `shapley_value()` | 3 | Shapley attribution values |
| `banzhaf_value()` | 3 | Banzhaf attribution values |

### Welfare Metrics

| Metric | Description |
|--------|-------------|
| UW | Utilitarian Welfare (sum of payoffs) |
| NW | Nash Welfare (geometric mean of payoffs) |
| NW+ | Nash Welfare on advantages (payoff - BATNA) |

---

## Metrics Reference (from LLM Meta-Game Paper)

### Welfare Functions

**Utilitarian Welfare (UW)**

The sum of players' payoffs:

```
UW := uâ‚ + uâ‚‚
```

Maximizing UW finds the most efficient outcome in terms of total value created.

**Nash Welfare (NW)**

The geometric mean of players' payoffs:

```
NW := (uâ‚ Â· uâ‚‚)^(1/2)
```

Nash welfare balances efficiency and fairness by giving weight to both players' outcomes. It is maximized when gains are distributed more equally.

**Nash Welfare on Advantages (NW+)**

Nash welfare computed on *advantages* (surplus above BATNA):

```
NW+ := (uâ‚âº Â· uâ‚‚âº)^(1/2)

where u_iâº = max{0, uáµ¢ âˆ’ báµ¢}
```

Here `báµ¢` is player i's BATNA (Best Alternative to Negotiated Agreement). NW+ measures the geometric mean of gains *above* each player's outside option, accounting for the possibility that a player may receive less than their BATNA.

### Fairness Metric

**EF1 (Envy-Free up to One Item)**

An allocation is **envy-free** if each player values their own bundle at least as much as the other's. Since envy-free allocations may not exist, we use EF1 as a relaxation:

An allocation is **EF1** if, for each player i, there exists an item in the other player's bundle which, if removed, would eliminate envy:

```
váµ¢ Â· aâ‚‹áµ¢ âˆ’ váµ¢ Â· aáµ¢ â‰¤ max_{k: aâ‚‹áµ¢,â‚– > 0} váµ¢,â‚–    for i âˆˆ {1, 2}
```

The **EF1 frequency** is the fraction of bargaining instances ending in ACCEPT that produce EF1 allocations.

### Individual Effectiveness

**Regret**

For a symmetric two-player game, the regret of strategy Ï€ at Nash equilibrium Ïƒ* is:

```
Regret(Ï€) := u(Ïƒ*) âˆ’ u(Ï€, Ïƒ*â‚‹áµ¢)
```

This measures how much worse strategy Ï€ performs compared to the equilibrium value when facing equilibrium opponents. Lower regret indicates a strategy closer to best-responding.

### Equilibrium Selection

**Maximum Entropy Nash Equilibrium (MENE)**

When multiple Nash equilibria exist, we select the one maximizing Shannon entropy:

```
Ïƒ* = argmax_{Ïƒ âˆˆ NE(G)} [âˆ’Ïƒ Â· ln(Ïƒ)]
```

This provides a unique, well-defined equilibrium that spreads probability mass across strategies when indifferent, avoiding arbitrary selection among equivalent equilibria.

### Fairness Metrics

| Function | Description |
|----------|-------------|
| `ef1_frequency()` | EF1 frequency per policy pair |
| `ef1_frequency_matrix()` | EF1 frequency as matrix |
| `aggregate_ef1_between_groups()` | Compare EF1 between policy groups |

### Solvers

| Solver | Description |
|--------|-------------|
| `"mene"` | Maximum Entropy Nash Equilibrium (MILP-based) |
| `"uniform"` | Uniform distribution (baseline) |

---

## Framework Overview

### Notation Reference

| Symbol | Description |
|--------|-------------|
| G = (N, (Sáµ¢), (uáµ¢)) | Normal-form game with players N, strategy sets Sáµ¢, utilities uáµ¢ |
| Äœ | Empirical game (payoffs estimated via simulation/data) |
| S | Full strategy universe (all available strategies/policies) |
| X âŠ† S | Baseline restricted strategy set (library) |
| sâ±¼ âˆˆ S âˆ– X | Candidate strategy not yet in X |
| sáµ¢ âˆˆ X | Incumbent strategy in X |
| Sâ†“X | Restriction operator (per-player: Xáµ¢ âŠ† Sáµ¢) |
| Äœ_{Sâ†“X} | Restricted empirical game induced by X |
| Ïƒ âˆˆ Î”(X) | Mixed strategy profile over X |
| Ïƒ_X | Equilibrium mixture computed on Äœ_{Sâ†“X} |
| S | Meta-strategy solver (MSS): Äœ_{Sâ†“X} â†¦ Ïƒ_X |
| BRáµ¢(Ïƒâ‚‹áµ¢) | Best-response correspondence for player i |
| Ïáµ¢á´³(Ïƒ) | Regret of player i at profile Ïƒ (in game G) |
| Î¦ | A metric functional, Î¦(Äœ_{Sâ†“X}, Ïƒ_X) (Regret, Utility, NW, UW, EF1, etc.) |

### Game-Theoretic Foundations

We model multi-agent evaluation as an empirical game Äœ = (N, (Sáµ¢), (Ã»áµ¢)) where:

- **N** â€” Set of player roles (typically N = {1, 2} for pairwise evaluation)
- **Sáµ¢** â€” Strategy set for player i (the set of available policies)
- **Ã»áµ¢** â€” Estimated utility function Ã»áµ¢ : âˆâ±¼âˆˆN Sâ±¼ â†’ â„
- **Ïƒáµ¢** â€” Mixed strategy for player i, where Ïƒáµ¢ âˆˆ Î”(Sáµ¢)
- **Ïƒâ‚‹áµ¢** â€” Strategy profile of all players except i

### Strategy Restriction

Following empirical game-theoretic analysis, we consider restricted strategy sets. Let Sâ†“X denote restriction to Xáµ¢ âŠ† Sáµ¢. The restricted empirical game is:

```
Äœ_{Sâ†“X} = (N, (Xáµ¢), (Ã»áµ¢))
```

In our framework:
- **X** represents the **baseline library** of policies
- **Xâº = X âˆª {sâ±¼}** represents the library after adding candidate sâ±¼

### EGTA Pipeline: Restrict â†’ Solve â†’ Evaluate

1. **Restricted-game construction:**
   ```
   Äœ_{Sâ†“X} := g(X)
   ```
   where g(Â·) denotes assembling/estimating the restricted payoff table on X.

2. **Solve for an equilibrium/solution mixture:**
   ```
   Ïƒ_X := S(Äœ_{Sâ†“X})
   ```

3. **Restricted game value functional:**
   ```
   W(X) := Î¦(Äœ_{Sâ†“X}, Ïƒ_X)
   ```
   an outcome induced by the solution concept and the game (e.g., welfare at equilibrium, fairness at equilibrium, exploitability).

4. **Make solver dependence explicit:**
   ```
   W_S(X) := Î¦(Äœ_{Sâ†“X}, S(Äœ_{Sâ†“X}))
   ```
   Define a solution concept S (MENE, affinity entropy, etc.).

### Equilibrium and Regret

A mixed strategy profile Ïƒ* is a **Nash equilibrium** if:

```
Ïƒ*áµ¢ âˆˆ bráµ¢(Ïƒ*â‚‹áµ¢)   âˆ€i âˆˆ N
```

where bráµ¢(Ïƒâ‚‹áµ¢) = argmax over Ïƒ'áµ¢ âˆˆ Î”(Sáµ¢) of uáµ¢(Ïƒ'áµ¢, Ïƒâ‚‹áµ¢).

Player i's **regret** in profile Ïƒ:

```
Ïá´³áµ¢(Ïƒ) = max_{s'áµ¢ âˆˆ Sáµ¢} uáµ¢(s'áµ¢, Ïƒâ‚‹áµ¢) âˆ’ uáµ¢(Ïƒáµ¢, Ïƒâ‚‹áµ¢)
```

The **minimum regret constrained profile** (MRCP) for a restricted game:

```
MRCP(G_{Sâ†“X}) = argmin_{Ïƒ âˆˆ Î”(X)} Î£áµ¢âˆˆN Ïá´³áµ¢(Ïƒ)
```

---

## Three Levels of Analysis

### Level 1: Direct Interaction Effect (No Re-Equilibration)

Level 1 measures direct interaction effects without ecosystem adaptation. Fix a baseline library X and its equilibrium Ïƒ_X. For each incumbent strategy sáµ¢ âˆˆ X, compare outcomes against candidate sâ±¼ versus typical equilibrium partners.

**Pairwise outcome** for strategy pair (sáµ¢, sâ‚‹áµ¢):

```
m(sáµ¢, sâ‚‹áµ¢) := E[Z | (sáµ¢, sâ‚‹áµ¢)]
```

This is an empirical average for some specified metric Z (e.g., payoff, welfare, fairness).

**Baseline equilibrium interaction value** for incumbent sáµ¢ âˆˆ X:

```
V_X(sáµ¢) := m(sáµ¢, Ïƒ_{Xâ‚‹áµ¢}) := E_{sâ‚‹áµ¢ ~ Ïƒ_{Xâ‚‹áµ¢}}[m(sáµ¢, sâ‚‹áµ¢)]
```

**Partner Lift** (direct, no adaptation):

```
PLâ‚(sáµ¢; sâ±¼ | X) := m(sáµ¢, sâ±¼) âˆ’ V_X(sáµ¢)
```

Interpretation: If incumbent sáµ¢ faces sâ±¼ instead of the equilibrium from Äœ_{Sâ†“X}, how much does the expected outcome change?

**Aggregations:**

| Aggregation | Description |
|-------------|-------------|
| Uniform average | (1/\|X\|) Î£ PLâ‚(sáµ¢; sâ±¼ \| X) over all sáµ¢ âˆˆ X |
| Equilibrium-weighted | Î£ Ïƒ_X(sáµ¢) Â· PLâ‚(sáµ¢; sâ±¼ \| X) over all sáµ¢ âˆˆ X |
| Worst-case | min over sáµ¢ âˆˆ X of PLâ‚(sáµ¢; sâ±¼ \| X) |
| Best-case | max over sáµ¢ âˆˆ X of PLâ‚(sáµ¢; sâ±¼ \| X) |

---

### Level 2: Restricted Game Change + Re-Equilibration

Level 2 measures ecosystem effects with strategic adaptation by adding sâ±¼ to the restricted game and re-solving.

**Expanded restricted game:**

```
Xâº := X âˆª {sâ±¼}
```

Re-estimate/assemble Äœ_{Sâ†“Xâº}, re-solve for Ïƒ_{Xâº}, then evaluate W(Xâº).

**Restricted game value functional:**

```
W(X) := Î¦(Äœ_{Sâ†“X}, Ïƒ_X)
```

where Î¦ is a metric functional (welfare, fairness, etc.) evaluated at equilibrium.

**Impact of adding sâ±¼ to X (ecosystem lift):**

```
Î”W(sâ±¼ | X) := W(Xâº) âˆ’ W(X)
```

**Incumbent value shift** under re-equilibration:

```
Î”_inc(sáµ¢; sâ±¼ | X) := V_{Xâº}(sáµ¢) âˆ’ V_X(sáµ¢)
```

**Equilibrium diagnostics:**

| Metric | Formula |
|--------|---------|
| Equilibrium shift | Î”Ïƒ(sâ±¼ \| X) := â€–Ïƒ_{Xâº} âˆ’ Ïƒ_Xâ€–â‚ |
| Entry mass | EntryMass(sâ±¼ \| X) := Ïƒ_{Xâº}(sâ±¼) |

---

### Level 3: Synergy-Aware Credit Attribution

Level 3 assigns credit across sub-ecosystems using cooperative game theory, averaging over many possible restricted games.

**Value function** over sub-libraries X âŠ† S:

```
v(X) := W(X) = Î¦(Äœ_{Sâ†“X}, Ïƒ_X), where Ïƒ_X = S(Äœ_{Sâ†“X})
```

**Banzhaf value:**

```
Î²(s) := E_{X âŠ† Sâˆ–{s}}[v(X âˆª {s}) âˆ’ v(X)]
     = (1/2^{|S|âˆ’1}) Î£_{X âŠ† Sâˆ–{s}} [v(X âˆª {s}) âˆ’ v(X)]
```

**Shapley value:**

```
Ï†(s) := E_â‰º[v(Pred_â‰º(s) âˆª {s}) âˆ’ v(Pred_â‰º(s))]
     = (1/|S|!) Î£_â‰º [v(Pred_â‰º(s) âˆª {s}) âˆ’ v(Pred_â‰º(s))]
```

where â‰º is a uniform random ordering of S, and Pred_â‰º(s) denotes the set of strategies preceding s under â‰º.

---

## Key Distinctions

| Aspect | Level 1 | Level 2 | Level 3 |
|--------|---------|---------|---------|
| Strategy set | Fixed X | Expanded Xâº | All subsets of S |
| Equilibrium | Ïƒ_X held fixed | Recomputed Ïƒ_{Xâº} | Recomputed per subset |
| Interpretation | Partner quality (direct effect) | Restricted game impact (with adaptation) | Synergy-aware attribution |

---

## Solver Sensitivity

Equilibrium selection affects all metrics. For solver ğ’®:

```
W_ğ’®(X) := W(ğ’®(G_{Sâ†“X}), G_{Sâ†“X})
```

Solver sensitivity:

```
Sens(ğ’®â‚, ğ’®â‚‚; X) := W_{ğ’®â‚}(X) âˆ’ W_{ğ’®â‚‚}(X)
```

---

## Development

```bash
# Run tests
uv run pytest

# Run tests with coverage
uv run pytest --cov=src/iterative_game_analysis

# Lint code
uv run ruff check .

# Type check
uv run mypy src/
```

## License

MIT
