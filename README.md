# Causal Meta-Game Analysis

A framework for analyzing multi-agent systems using Structural Causal Models (SCM) and do-calculus. This framework provides rigorous causal semantics for evaluating policies in meta-games across three levels of analysis.

## Installation

Requires Python 3.10+. Install using [uv](https://github.com/astral-sh/uv):

```bash
# Clone the repository
git clone https://github.com/your-username/Causal-Game-Analysis.git
cd Causal-Game-Analysis

# Sync dependencies (creates venv automatically)
uv sync

# For development (includes testing and linting tools)
uv sync --extra dev
```

## Quick Start

### Basic Usage

```python
import pandas as pd
import numpy as np
from causal_game_analysis import (
    MetaGame,
    Bootstrap,
    level1_analysis,
    ecosystem_lift,
    shapley_value,
)

# Create or load cross-play data
# Format: DataFrame with columns (policy_i, policy_j, outcome)
df = pd.DataFrame([
    {"policy_i": "alice", "policy_j": "bob", "outcome": 0.8},
    {"policy_i": "alice", "policy_j": "carol", "outcome": 0.6},
    {"policy_i": "bob", "policy_j": "alice", "outcome": 0.7},
    # ... more cross-play results
])

# Build meta-game from raw data
game = MetaGame.from_dataframe(df)

# Compute equilibrium
sigma = game.solve("mene")  # Max-entropy Nash equilibrium
print("Equilibrium:", dict(zip(game.policies, sigma)))
```

### Level 1: Partner Lift Analysis

```python
# Evaluate a candidate policy against a baseline
result = level1_analysis(
    metagame=game,
    baseline_policies=["alice", "bob"],
    candidate="carol",
    solver="mene"
)

print("Partner Lift per incumbent:", result["per_incumbent"])
print("Uniform average:", result["uniform_avg"])
print("Equilibrium-weighted average:", result["equilibrium_avg"])
print("Worst-case:", result["min"])
print("Best-case:", result["max"])
```

### Level 2: Ecosystem Lift Analysis

```python
# Measure ecosystem impact with re-equilibration
result = ecosystem_lift(
    metagame=game,
    baseline_policies=["alice", "bob"],
    candidate="carol",
    solver="mene",
    welfare_fn="utilitarian"  # or "nash", "egalitarian"
)

print("Ecosystem lift:", result["delta_eco"])
print("Entry mass:", result["entry_mass"])
print("Equilibrium shift:", result["equilibrium_shift"])
print("Incumbent value shifts:", result["incumbent_shifts"])
```

### Level 3: Shapley Attribution

```python
# Compute Shapley values for ecosystem attribution
def value_fn(policies):
    if len(policies) < 2:
        return 0.0
    sub_game = game.subset(policies)
    sigma = sub_game.solve("mene")
    return sub_game.welfare(sigma, "utilitarian")

shapley = shapley_value(game.policies, value_fn)
print("Shapley values:", shapley)

# Or Banzhaf values
from causal_game_analysis import banzhaf_value
banzhaf = banzhaf_value(game.policies, value_fn)
```

### Bootstrap for Uncertainty Quantification

```python
# Bootstrap resampling for confidence intervals
bootstrap = Bootstrap(df, n_samples=1000, seed=42)

# Run any analysis on bootstrap samples
def analyze(g):
    return level1_analysis(g, ["alice", "bob"], "carol")["uniform_avg"]

results = bootstrap.run(analyze, progress=True)

# Get confidence interval
lower, median, upper = Bootstrap.confidence_interval(results, alpha=0.05)
print(f"95% CI: [{lower:.3f}, {upper:.3f}]")
```

### EF1 Fairness Analysis (for Bargaining)

```python
from causal_game_analysis import ef1_frequency_matrix, aggregate_ef1_between_groups

# If your data includes EF1 indicator column
df_with_ef1 = pd.DataFrame([
    {"policy_i": "gpt4", "policy_j": "claude", "outcome": 0.8, "ef1": 1},
    {"policy_i": "gpt4", "policy_j": "llama", "outcome": 0.6, "ef1": 0},
    # ...
])

# EF1 frequency matrix
ef1_matrix, policies = ef1_frequency_matrix(df_with_ef1)

# Compare EF1 between groups
ef1_stats = aggregate_ef1_between_groups(
    df_with_ef1,
    group_a=["gpt4", "claude"],
    group_b=["llama", "mistral"]
)
print("EF1 frequency (Group A vs B):", ef1_stats["a_vs_b"])
print("EF1 frequency (within Group A):", ef1_stats["within_a"])
```

### Direct Matrix Construction

```python
# If you already have a payoff matrix
payoff_matrix = np.array([
    [0.5, 0.0, 1.0],  # rock
    [1.0, 0.5, 0.0],  # paper
    [0.0, 1.0, 0.5],  # scissors
])

game = MetaGame(
    policies=["rock", "paper", "scissors"],
    payoff_matrix=payoff_matrix
)
```

## API Reference

### Core Classes

| Class | Description |
|-------|-------------|
| `MetaGame` | Empirical meta-game representation with payoff matrix |
| `Bootstrap` | Bootstrap resampling for uncertainty quantification |

### Analysis Functions

| Function | Level | Description |
|----------|-------|-------------|
| `level1_analysis()` | 1 | Partner Lift (no re-equilibration) |
| `ecosystem_lift()` | 2 | Ecosystem Lift (with re-equilibration) |
| `shapley_value()` | 3 | Shapley attribution values |
| `banzhaf_value()` | 3 | Banzhaf attribution values |

### Fairness Metrics

| Function | Description |
|----------|-------------|
| `ef1_frequency()` | EF1 frequency per policy pair |
| `ef1_frequency_matrix()` | EF1 frequency as matrix |
| `aggregate_ef1_between_groups()` | Compare EF1 between policy groups |

### Solvers

| Solver | Description |
|--------|-------------|
| `"mene"` | Maximum Entropy Nash Equilibrium (MILP-based) |
| `"uniform"` | Uniform distribution (baseline) |

---

## Framework Overview

### Game-Theoretic Foundations

We model multi-agent evaluation as an empirical game Äœ = (N, (Sáµ¢), (Ã»áµ¢)) where:

- **N** â€” Set of player roles (typically N = {1, 2} for pairwise evaluation)
- **Sáµ¢** â€” Strategy set for player i (the set of available policies)
- **Ã»áµ¢** â€” Estimated utility function Ã»áµ¢ : âˆâ±¼âˆˆN Sâ±¼ â†’ â„
- **Ïƒáµ¢** â€” Mixed strategy for player i, where Ïƒáµ¢ âˆˆ Î”(Sáµ¢)
- **Ïƒâ‚‹áµ¢** â€” Strategy profile of all players except i

### Strategy Restriction

Following empirical game-theoretic analysis, we consider restricted strategy sets. Let Sâ†“X denote restriction to Xáµ¢ âŠ† Sáµ¢. The restricted empirical game is:

```
Äœ_{Sâ†“X} = (N, (Xáµ¢), (Ã»áµ¢))
```

In our framework:
- **X** represents the **baseline library** of policies
- **Xâº = X âˆª {sâ±¼}** represents the library after adding candidate sâ±¼

### Equilibrium and Regret

A mixed strategy profile Ïƒ* is a **Nash equilibrium** if:

```
Ïƒ*áµ¢ âˆˆ bráµ¢(Ïƒ*â‚‹áµ¢)   âˆ€i âˆˆ N
```

where bráµ¢(Ïƒâ‚‹áµ¢) = argmax over Ïƒ'áµ¢ âˆˆ Î”(Sáµ¢) of uáµ¢(Ïƒ'áµ¢, Ïƒâ‚‹áµ¢).

Player i's **regret** in profile Ïƒ:

```
Ïá´³áµ¢(Ïƒ) = max_{s'áµ¢ âˆˆ Sáµ¢} uáµ¢(s'áµ¢, Ïƒâ‚‹áµ¢) âˆ’ uáµ¢(Ïƒáµ¢, Ïƒâ‚‹áµ¢)
```

The **minimum regret constrained profile** (MRCP) for a restricted game:

```
MRCP(G_{Sâ†“X}) = argmin_{Ïƒ âˆˆ Î”(X)} Î£áµ¢âˆˆN Ïá´³áµ¢(Ïƒ)
```

---

## Three Levels of Analysis

### Level 1: Interaction-Level (No Re-Equilibration)

Level 1 measures direct interaction effects without ecosystem adaptation. Fix a baseline library X and its equilibrium Ïƒ_X. For each incumbent strategy sáµ¢ âˆˆ X, compare outcomes against candidate sâ±¼ versus typical equilibrium partners.

**Baseline expected utility** for incumbent sáµ¢ âˆˆ X:

```
U_X(sáµ¢) := Î£_{s âˆˆ X} Ïƒ_X(s) Â· u(sáµ¢, s)
```

**Partner Lift** (strategy-specific):

```
PLâ‚(sáµ¢; sâ±¼ | X) := u(sáµ¢, sâ±¼) âˆ’ U_X(sáµ¢)
```

**Aggregations:**

| Aggregation | Description |
|-------------|-------------|
| Uniform average | (1/\|X\|) Î£ PLâ‚(sáµ¢; sâ±¼ \| X) over all sáµ¢ âˆˆ X |
| Equilibrium-weighted | Î£ Ïƒ_X(sáµ¢) Â· PLâ‚(sáµ¢; sâ±¼ \| X) over all sáµ¢ âˆˆ X |
| Worst-case | min over sáµ¢ âˆˆ X of PLâ‚(sáµ¢; sâ±¼ \| X) |
| Best-case | max over sáµ¢ âˆˆ X of PLâ‚(sáµ¢; sâ±¼ \| X) |

---

### Level 2: Ecosystem-Level (Re-Equilibration)

Level 2 measures ecosystem effects with strategic adaptation. Expand the strategy set to Xâº = X âˆª {sâ±¼}, compute new equilibrium Ïƒ_{Xâº}, and compare welfare.

**Welfare function** over profile Ïƒ in game G:

```
W(Ïƒ, G) = f((uáµ¢(Ïƒ))_{i âˆˆ N})
```

Common choices: utilitarian (Î£áµ¢ uáµ¢), Nash product (Î áµ¢ uáµ¢), egalitarian (mináµ¢ uáµ¢).

**Ecosystem lift:**

```
Î”_eco(sâ±¼ | X) := W(Ïƒ_{Xâº}, G_{Sâ†“Xâº}) âˆ’ W(Ïƒ_X, G_{Sâ†“X})
```

**Incumbent value shift** under re-equilibration:

```
Î”_inc(sáµ¢; sâ±¼ | X) := U_{Xâº}(sáµ¢) âˆ’ U_X(sáµ¢)
```

**Equilibrium diagnostics:**

| Metric | Description |
|--------|-------------|
| Equilibrium shift | â€–Ïƒ_{Xâº} âˆ’ Ïƒ_Xâ€–â‚ (L1 norm, restricted to X) |
| Entry mass | Ïƒ_{Xâº}(sâ±¼) |

---

### Level 3: Ecosystem Attribution (Shapley/Banzhaf)

Level 3 assigns credit across sub-ecosystems using cooperative game theory. Define value function:

```
v(X) := W(Ïƒ_X, G_{Sâ†“X})
```

**Shapley value:**

```
Ï†(s) := (1/|S|!) Î£_{orderings â‰º} [v(Pred_â‰º(s) âˆª {s}) âˆ’ v(Pred_â‰º(s))]
```

**Banzhaf value:**

```
Î²(s) := (1/2^{|S|âˆ’1}) Î£_{X âŠ† Sâˆ–{s}} [v(X âˆª {s}) âˆ’ v(X)]
```

---

## Key Distinctions

| Aspect | Level 1 | Level 2 |
|--------|---------|---------|
| Strategy set | Fixed X | Expanded Xâº |
| Equilibrium | Baseline Ïƒ_X unchanged | Recomputed Ïƒ_{Xâº} |
| Interpretation | Partner quality | Ecosystem impact |

---

## Solver Sensitivity

Equilibrium selection affects all metrics. For solver ð’®:

```
W_ð’®(X) := W(ð’®(G_{Sâ†“X}), G_{Sâ†“X})
```

Solver sensitivity:

```
Sens(ð’®â‚, ð’®â‚‚; X) := W_{ð’®â‚}(X) âˆ’ W_{ð’®â‚‚}(X)
```

---

## RL Training Framework

This repository also includes a comprehensive RL training framework for the CUDA-accelerated bargaining game environment.

### Quick Start

```bash
# Install CUDA environment (requires NVIDIA GPU)
cd cuda_bargain && pip install -e .

# Train a PPO agent
python scripts/train_ppo_bargain.py --num-envs 4096 --total-timesteps 5000000

# View results
python scripts/view_results.py --list
```

### Available Algorithms

| Algorithm | Type | Command |
|-----------|------|---------|
| **PPO** | Self-play | `python scripts/train_ppo_bargain.py` |
| **NFSP** | Self-play | `python scripts/train_nfsp_bargain.py` |
| **Sampled CFR** | Equilibrium | `python scripts/train_sampled_cfr.py` |
| **PSRO** | Population | `python scripts/train_psro.py` |
| **MAPPO** | Self-play | `python scripts/train_mappo.py` |
| **FCP** | Population | `python scripts/train_fcp.py` |

### Training with Logging

```bash
# With Weights & Biases
python scripts/train_ppo_bargain.py \
    --num-envs 4096 \
    --total-timesteps 10000000 \
    --wandb \
    --wandb-project causal-bargain
```

### Hyperparameter Sweeps

```bash
# Using W&B Sweeps
wandb sweep sweeps/sweep_ppo.yaml
wandb agent <sweep_id>

# Using Python script
python scripts/hyperparameter_sweep.py --algorithm ppo --method random --num-trials 50
```

### Results Management

All training results are automatically saved with:
- Trained neural network weights
- Full hyperparameter configuration
- Training metrics history
- Final evaluation results

```bash
# View all runs
python scripts/view_results.py --list

# Compare algorithms
python scripts/view_results.py --leaderboard

# Export to CSV
python scripts/view_results.py --export results.csv
```

### Documentation

- **[RL Training Guide](rl_training/README.md)** - Full documentation for training algorithms
- **[Scripts Reference](scripts/README.md)** - Command-line script usage
- **[Sweeps Guide](sweeps/README.md)** - Hyperparameter optimization

---

## Development

```bash
# Run tests
uv run pytest

# Run tests with coverage
uv run pytest --cov=src/causal_game_analysis

# Lint code
uv run ruff check .

# Type check
uv run mypy src/
```

## License

MIT
